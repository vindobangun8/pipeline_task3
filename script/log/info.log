2025-11-13 09:00:12,673 - INFO - ===== Start Extracting education_status data =====
2025-11-13 09:00:12,779 - INFO - ===== Finish Extracting education_status data =====
2025-11-13 09:01:23,255 - INFO - ===== Start Extracting marital_status data =====
2025-11-13 09:01:23,522 - INFO - ===== Finish Extracting marital_status data =====
2025-11-13 09:02:51,953 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-11-13 09:02:52,233 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-13 09:08:07,847 - INFO - ===== Start Extracting new_bank_transaction.csv data =====
2025-11-13 09:08:08,656 - ERROR - ====== Failed to Extract Data ======
2025-11-13 09:08:08,657 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/new_bank_transaction.csv.csv.
2025-11-13 09:08:22,718 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-11-13 09:08:24,748 - ERROR - ====== Failed to Extract Data ======
2025-11-13 09:08:24,748 - ERROR - An error occurred while calling o63.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (pyspark executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv at 0 exp: -427772641 got: 1092707063
	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)
	at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.spark.SparkException: Encountered error while reading file file:///home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv at 0 exp: -427772641 got: 1092707063
	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)
	at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
	... 22 more

2025-11-13 09:14:25,301 - INFO - ===== Start Extracting education_status data =====
2025-11-13 09:14:26,833 - INFO - ===== Finish Extracting education_status data =====
2025-11-13 09:14:31,872 - INFO - ===== Start Extracting marital_status data =====
2025-11-13 09:14:31,933 - INFO - ===== Finish Extracting marital_status data =====
2025-11-13 09:14:35,212 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-11-13 09:14:35,303 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-13 09:14:41,739 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-11-13 09:14:43,594 - ERROR - ====== Failed to Extract Data ======
2025-11-13 09:14:43,594 - ERROR - An error occurred while calling o46.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (pyspark executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv at 0 exp: -427772641 got: 1092707063
	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)
	at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.spark.SparkException: Encountered error while reading file file:///home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv at 0 exp: -427772641 got: 1092707063
	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)
	at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
	... 22 more

2025-11-13 09:15:45,166 - INFO - ===== Start Extracting education_status data =====
2025-11-13 09:15:46,563 - INFO - ===== Finish Extracting education_status data =====
2025-11-13 09:15:51,315 - INFO - ===== Start Extracting marital_status data =====
2025-11-13 09:15:51,433 - INFO - ===== Finish Extracting marital_status data =====
2025-11-13 09:15:52,228 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-11-13 09:15:52,324 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-13 09:15:57,629 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-11-13 09:15:59,435 - ERROR - ====== Failed to Extract Data ======
2025-11-13 09:15:59,436 - ERROR - An error occurred while calling o46.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (pyspark executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv at 0 exp: -427772641 got: 1092707063
	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)
	at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.spark.SparkException: Encountered error while reading file file:///home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv at 0 exp: -427772641 got: 1092707063
	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)
	at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
	... 22 more

2025-11-13 09:19:04,577 - INFO - ===== Start Extracting education_status data =====
2025-11-13 09:19:05,992 - INFO - ===== Finish Extracting education_status data =====
2025-11-13 09:19:10,855 - INFO - ===== Start Extracting marital_status data =====
2025-11-13 09:19:10,920 - INFO - ===== Finish Extracting marital_status data =====
2025-11-13 09:19:12,437 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-11-13 09:19:12,524 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-13 09:19:15,009 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-11-13 09:19:16,823 - ERROR - ====== Failed to Extract Data ======
2025-11-13 09:19:16,823 - ERROR - An error occurred while calling o47.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (pyspark executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv at 0 exp: -427772641 got: 1092707063
	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)
	at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.spark.SparkException: Encountered error while reading file file:///home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv at 0 exp: -427772641 got: 1092707063
	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)
	at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
	... 22 more

2025-11-13 09:20:08,851 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-11-13 09:20:09,327 - ERROR - ====== Failed to Extract Data ======
2025-11-13 09:20:09,328 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/home/jovyan/work/data/new_bank_transaction.csv.
2025-11-13 09:20:47,181 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-11-13 09:20:47,233 - ERROR - ====== Failed to Extract Data ======
2025-11-13 09:20:47,234 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/home/jovyan/work/data/new_bank_transaction.csv.
2025-11-13 09:21:11,484 - INFO - ===== Start Extracting education_status data =====
2025-11-13 09:21:12,875 - INFO - ===== Finish Extracting education_status data =====
2025-11-13 09:21:17,305 - INFO - ===== Start Extracting marital_status data =====
2025-11-13 09:21:17,365 - INFO - ===== Finish Extracting marital_status data =====
2025-11-13 09:21:18,782 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-11-13 09:21:18,884 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-13 09:21:21,475 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-11-13 09:21:23,375 - ERROR - ====== Failed to Extract Data ======
2025-11-13 09:21:23,376 - ERROR - An error occurred while calling o47.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (pyspark executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv at 0 exp: -427772641 got: 1092707063
	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)
	at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.spark.SparkException: Encountered error while reading file file:///home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00003-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv at 0 exp: -427772641 got: 1092707063
	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)
	at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
	... 22 more

2025-11-13 09:43:46,505 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-11-13 09:43:48,189 - ERROR - ====== Failed to Extract Data ======
2025-11-13 09:43:48,189 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00000-a06d3523-1333-49aa-a868-f8e62c74973b-c000.
2025-11-13 09:44:38,583 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-11-13 09:44:44,457 - ERROR - ====== Failed to Extract Data ======
2025-11-13 09:44:44,458 - ERROR - An error occurred while calling o27.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (pyspark executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///home/jovyan/work/data/new_bank_transaction.csv/part-00000-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00000-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv at 0 exp: -1024389319 got: -409770179
	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)
	at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.spark.SparkException: Encountered error while reading file file:///home/jovyan/work/data/new_bank_transaction.csv/part-00000-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/home/jovyan/work/data/new_bank_transaction.csv/part-00000-a06d3523-1333-49aa-a868-f8e62c74973b-c000.csv at 0 exp: -1024389319 got: -409770179
	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)
	at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
	... 22 more

2025-11-13 10:05:36,464 - INFO - Note: NumExpr detected 12 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-11-13 10:05:36,468 - INFO - NumExpr defaulting to 8 threads.
2025-11-13 10:23:13,983 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-11-13 10:23:16,770 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-13 12:40:52,808 - INFO - ===== Start Extracting education_status data =====
2025-11-13 12:40:55,740 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 03:55:10,104 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 03:55:11,996 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 03:55:16,201 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 03:55:16,266 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 03:55:19,253 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 03:55:19,355 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 03:55:22,239 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 03:55:23,436 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 04:10:01,198 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 04:10:02,021 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 05:09:43,415 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 05:09:43,716 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 05:12:00,514 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 05:12:00,564 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 05:53:03,979 - INFO - ===== Start Transform Customer Data =====
2025-11-14 05:53:04,040 - ERROR - ===== Failed Transform Customer Data =====
2025-11-14 05:53:04,041 - ERROR - name 'cust_copy' is not defined
2025-11-14 05:53:22,406 - INFO - ===== Start Transform Customer Data =====
2025-11-14 05:53:22,433 - ERROR - ===== Failed Transform Customer Data =====
2025-11-14 05:53:22,434 - ERROR - name 'cust_copy' is not defined
2025-11-14 05:54:00,523 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 05:54:02,235 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 05:54:06,330 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 05:54:06,395 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 05:54:06,562 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 05:54:06,634 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 05:54:07,692 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 05:54:08,652 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 05:54:27,586 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 05:54:27,640 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 05:54:36,524 - INFO - ===== Start Transform Customer Data =====
2025-11-14 05:54:36,579 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 05:55:48,624 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 05:55:49,097 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 05:56:59,544 - INFO - ===== Start Transform Customer Data =====
2025-11-14 05:56:59,622 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 06:36:59,193 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 06:36:59,812 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 06:38:56,969 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 06:38:57,902 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 06:39:01,097 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 06:39:01,157 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 06:39:01,259 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 06:39:01,341 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 06:39:14,376 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 06:39:15,203 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 06:40:41,196 - INFO - ===== Start Transform Customer Data =====
2025-11-14 06:40:41,248 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 09:01:17,884 - ERROR - No such comm target registered: jupyter.widget.control
2025-11-14 09:01:17,902 - WARNING - No such comm: f556a236-ea27-4929-b472-0eac543b6faf
2025-11-14 09:15:20,587 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 09:15:22,085 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 09:15:25,755 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 09:15:25,812 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 09:15:25,974 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 09:15:26,064 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 09:15:26,579 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 09:15:27,728 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 09:15:33,890 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 09:15:33,926 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 09:15:34,921 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:15:34,972 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 09:15:38,377 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:15:38,404 - ERROR - ===== Failed Transform Transaction Data =====
2025-11-14 09:15:38,405 - ERROR - name 'date_format' is not defined
2025-11-14 09:16:52,511 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 09:16:53,632 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 09:16:56,500 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 09:16:56,550 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 09:16:56,706 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 09:16:56,766 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 09:16:57,177 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 09:16:58,343 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 09:17:03,632 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 09:17:03,671 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 09:17:04,758 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:17:04,826 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 09:17:08,270 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:17:08,331 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 09:18:37,639 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:18:37,730 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 09:34:57,298 - INFO - ===== Start Load data to the database =====
2025-11-14 09:34:57,396 - ERROR - ===== Failed Load data to the database =====
2025-11-14 09:34:57,397 - ERROR - name 'jdbc_url' is not defined
2025-11-14 09:35:29,248 - INFO - ===== Start Load data to the database =====
2025-11-14 09:35:29,254 - ERROR - ===== Failed Load data to the database =====
2025-11-14 09:35:29,255 - ERROR - name 'jdbc_url' is not defined
2025-11-14 09:35:39,836 - INFO - ===== Start Load data to the database =====
2025-11-14 09:35:39,840 - ERROR - ===== Failed Load data to the database =====
2025-11-14 09:35:39,841 - ERROR - name 'jdbc_url' is not defined
2025-11-14 09:35:58,331 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 09:35:59,721 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 09:36:02,620 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 09:36:02,695 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 09:36:02,827 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 09:36:02,926 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 09:36:03,437 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 09:36:04,762 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 09:36:10,626 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 09:36:10,666 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 09:36:11,675 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:36:11,742 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 09:36:15,524 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:36:15,614 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 09:36:15,860 - INFO - ===== Start Load data to the database =====
2025-11-14 09:36:15,864 - ERROR - ===== Failed Load data to the database =====
2025-11-14 09:36:15,864 - ERROR - name 'warehous_url' is not defined
2025-11-14 09:36:52,447 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 09:36:53,471 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 09:36:56,397 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 09:36:56,496 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 09:36:56,766 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 09:36:56,833 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 09:36:57,332 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 09:36:58,423 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 09:37:04,036 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 09:37:04,098 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 09:37:05,122 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:37:05,185 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 09:37:08,742 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:37:08,837 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 09:37:09,135 - INFO - ===== Start Load data to the database =====
2025-11-14 09:37:09,173 - ERROR - ===== Failed Load data to the database =====
2025-11-14 09:37:09,173 - ERROR - Unknown save mode: insert. Accepted save modes are 'overwrite', 'append', 'ignore', 'error', 'errorifexists', 'default'.
2025-11-14 09:37:42,060 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 09:37:43,225 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 09:37:46,100 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 09:37:46,152 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 09:37:46,353 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 09:37:46,421 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 09:37:46,853 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 09:37:47,940 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 09:37:52,886 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 09:37:52,924 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 09:37:53,992 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:37:54,092 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 09:37:57,413 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:37:57,462 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 09:37:57,688 - INFO - ===== Start Load data to the database =====
2025-11-14 09:37:57,752 - ERROR - ===== Failed Load data to the database =====
2025-11-14 09:37:57,752 - ERROR - An error occurred while calling o293.jdbc.
: org.postgresql.util.PSQLException: Unable to parse URL jdbc:postgresql://data_warehouse_container:warehouse/5432
	at org.postgresql.Driver.connect(Driver.java:280)
	at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
	at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)
	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)
	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)

2025-11-14 09:40:34,448 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 09:40:35,836 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 09:40:39,478 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 09:40:39,550 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 09:40:39,714 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 09:40:39,795 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 09:40:40,295 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 09:40:41,380 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 09:40:47,065 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 09:40:47,110 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 09:40:48,217 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:40:48,280 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 09:40:51,603 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:40:51,662 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 09:40:51,884 - INFO - ===== Start Load data to the database =====
2025-11-14 09:40:51,962 - ERROR - ===== Failed Load data to the database =====
2025-11-14 09:40:51,962 - ERROR - An error occurred while calling o293.jdbc.
: org.postgresql.util.PSQLException: FATAL: database "warehouse" does not exist
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.readStartupMessages(QueryExecutorImpl.java:2825)
	at org.postgresql.core.v3.QueryExecutorImpl.<init>(QueryExecutorImpl.java:175)
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:313)
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)
	at org.postgresql.Driver.makeConnection(Driver.java:443)
	at org.postgresql.Driver.connect(Driver.java:297)
	at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
	at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)
	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)
	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)

2025-11-14 09:41:41,463 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 09:41:42,551 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 09:41:45,303 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 09:41:45,357 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 09:41:45,513 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 09:41:45,567 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 09:41:45,957 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 09:41:47,040 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 09:41:52,094 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 09:41:52,133 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 09:41:53,252 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:41:53,337 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 09:41:56,909 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:41:56,978 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 09:41:57,205 - INFO - ===== Start Load data to the database =====
2025-11-14 09:41:57,491 - INFO - ===== Finish Load data to the database =====
2025-11-14 09:59:17,911 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 09:59:19,186 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 09:59:22,844 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 09:59:22,907 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 09:59:23,075 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 09:59:23,149 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 09:59:23,697 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 09:59:25,072 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 09:59:32,194 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 09:59:32,248 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 09:59:33,346 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:59:33,422 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 09:59:37,357 - INFO - ===== Start Transform Customer Data =====
2025-11-14 09:59:37,414 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 09:59:38,174 - ERROR - ===== Failed Load data to the database =====
2025-11-14 09:59:38,174 - ERROR - name 'create_engine' is not defined
2025-11-14 10:01:06,114 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 10:01:07,478 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 10:01:10,901 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 10:01:10,951 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 10:01:11,082 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 10:01:11,146 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 10:01:11,620 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 10:01:12,672 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 10:01:21,048 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 10:01:21,086 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 10:01:22,043 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:01:22,103 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 10:01:25,560 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:01:25,603 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 10:01:26,145 - ERROR - ===== Failed Load data to the database =====
2025-11-14 10:01:26,145 - ERROR - 'function' object has no attribute 'connect'
2025-11-14 10:03:03,549 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 10:03:04,834 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 10:03:08,384 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 10:03:08,467 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 10:03:08,647 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 10:03:08,701 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 10:03:09,241 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 10:03:10,358 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 10:03:17,552 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 10:03:17,597 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 10:03:18,703 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:03:18,777 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 10:03:22,078 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:03:22,131 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 10:03:22,585 - ERROR - ===== Failed Load data to the database =====
2025-11-14 10:03:22,586 - ERROR - 'function' object has no attribute 'connect'
2025-11-14 10:04:19,747 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 10:04:20,891 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 10:04:23,903 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 10:04:23,956 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 10:04:24,091 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 10:04:24,150 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 10:04:24,588 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 10:04:25,692 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 10:04:31,663 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 10:04:31,698 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 10:04:32,621 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:04:32,679 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 10:04:36,029 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:04:36,082 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 10:04:36,399 - ERROR - ===== Failed Load data to the database =====
2025-11-14 10:04:36,400 - ERROR - (psycopg2.OperationalError) connection to server at "localhost" (::1), port 5439 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5439 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
2025-11-14 10:06:47,533 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 10:06:49,023 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 10:06:52,262 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 10:06:52,319 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 10:06:52,476 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 10:06:52,543 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 10:06:53,109 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 10:06:54,347 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 10:07:00,311 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 10:07:00,349 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 10:07:01,405 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:07:01,464 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 10:07:04,898 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:07:04,945 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 10:07:05,361 - ERROR - ===== Failed Load data to the database =====
2025-11-14 10:07:05,361 - ERROR - name 'text' is not defined
2025-11-14 10:09:09,985 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 10:09:11,157 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 10:09:13,829 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 10:09:13,886 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 10:09:14,023 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 10:09:14,084 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 10:09:14,422 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 10:09:15,566 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 10:09:20,860 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 10:09:20,893 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 10:09:21,831 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:09:21,891 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 10:09:25,057 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:09:25,112 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 10:09:25,557 - ERROR - ===== Failed Load data to the database =====
2025-11-14 10:09:25,557 - ERROR - (psycopg2.errors.FeatureNotSupported) cannot truncate a table referenced in a foreign key constraint
DETAIL:  Table "marketing_campaign_deposit" references "education_status".
HINT:  Truncate table "marketing_campaign_deposit" at the same time, or use TRUNCATE ... CASCADE.

[SQL: TRUNCATE TABLE education_status]
(Background on this error at: https://sqlalche.me/e/20/tw8g)
2025-11-14 10:14:39,166 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 10:14:41,041 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 10:14:45,512 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 10:14:45,607 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 10:14:45,884 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 10:14:45,978 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 10:14:46,728 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 10:14:48,014 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 10:14:56,222 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 10:14:56,283 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 10:14:57,240 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:14:57,306 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 10:15:00,505 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:15:00,545 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 10:15:01,028 - INFO - ===== Start Load data to the database =====
2025-11-14 10:15:01,246 - INFO - ===== Finish Load data to the database =====
2025-11-14 10:19:05,737 - INFO - ===== Start Load data to the database =====
2025-11-14 10:19:06,185 - INFO - ===== Finish Load data to the database =====
2025-11-14 10:19:49,340 - INFO - ===== Start Load data to the database =====
2025-11-14 10:19:52,431 - INFO - ===== Finish Load data to the database =====
2025-11-14 10:20:47,641 - INFO - ===== Start Load data to the database =====
2025-11-14 10:20:49,449 - INFO - ===== Finish Load data to the database =====
2025-11-14 10:21:12,994 - ERROR - ===== Failed Load data to the database =====
2025-11-14 10:21:12,995 - ERROR - (psycopg2.errors.UndefinedTable) relation "transaction" does not exist

[SQL: TRUNCATE TABLE transaction Cascade]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-11-14 10:21:25,873 - INFO - ===== Start Load data to the database =====
2025-11-14 10:21:26,560 - ERROR - ===== Failed Load data to the database =====
2025-11-14 10:21:26,560 - ERROR - An error occurred while calling o315.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 99.0 failed 1 times, most recent failure: Lost task 5.0 in stage 99.0 (TID 119) (pyspark executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO transactions ("transaction_id","customer_id","transaction_date","transaction_time","transaction_amount") VALUES ('T89544','C1010014','2016-08-01 +00'::date,'15:44:51',1205.00) was aborted: ERROR: insert or update on table "transactions" violates foreign key constraint "transactions_customer_id_fkey"
  Detail: Key (customer_id)=(C1010014) is not present in table "customers".  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.postgresql.util.PSQLException: ERROR: insert or update on table "transactions" violates foreign key constraint "transactions_customer_id_fkey"
  Detail: Key (customer_id)=(C1010014) is not present in table "customers".
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO transactions ("transaction_id","customer_id","transaction_date","transaction_time","transaction_amount") VALUES ('T89544','C1010014','2016-08-01 +00'::date,'15:44:51',1205.00) was aborted: ERROR: insert or update on table "transactions" violates foreign key constraint "transactions_customer_id_fkey"
  Detail: Key (customer_id)=(C1010014) is not present in table "customers".  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.postgresql.util.PSQLException: ERROR: insert or update on table "transactions" violates foreign key constraint "transactions_customer_id_fkey"
  Detail: Key (customer_id)=(C1010014) is not present in table "customers".
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

2025-11-14 10:27:09,414 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:27:09,667 - ERROR - ===== Failed Transform Transaction Data =====
2025-11-14 10:27:09,668 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `TransactionID` cannot be resolved. Did you mean one of the following? [`transaction_id`, `transaction_date`, `transaction_time`, `transaction_amount`, `customer_id`].;
'Project ['TransactionID, 'CustomerID, 'TransactionDate, 'TransactionTime, 'TransactionAmount (INR)]
+- Project [transaction_id#1231, customer_id#1228, transaction_date#1238, transaction_time#1244, cast(transaction_amount#1232 as decimal(10,2)) AS transaction_amount#1250]
   +- Project [transaction_id#1231, customer_id#1228, transaction_date#1238, date_format(to_timestamp(lpad(transaction_time#1229, 6, 0), Some(HHmmss), TimestampType, Some(Etc/UTC), false), HH:mm:ss, Some(Etc/UTC)) AS transaction_time#1244, transaction_amount#1232]
      +- Project [transaction_id#1231, customer_id#1228, to_date(transaction_date#1230, Some(d/M/yy), Some(Etc/UTC), false) AS transaction_date#1238, transaction_time#1229, transaction_amount#1232]
         +- Project [TransactionID#188 AS transaction_id#1231, CustomerID#189 AS customer_id#1228, TransactionDate#194 AS transaction_date#1230, TransactionTime#195 AS transaction_time#1229, TransactionAmount (INR)#196 AS transaction_amount#1232]
            +- Project [TransactionID#188, CustomerID#189, TransactionDate#194, TransactionTime#195, TransactionAmount (INR)#196]
               +- Project [TransactionID#188, CustomerID#189, TransactionDate#194, TransactionTime#195, TransactionAmount (INR)#196]
                  +- Relation [TransactionID#188,CustomerID#189,CustomerDOB#190,CustGender#191,CustLocation#192,CustAccountBalance#193,TransactionDate#194,TransactionTime#195,TransactionAmount (INR)#196] csv

2025-11-14 10:27:54,013 - INFO - ===== Start Transform Customer Data =====
2025-11-14 10:27:54,061 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 10:58:40,123 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 10:58:40,539 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 11:02:59,107 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 11:03:00,255 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 11:03:03,012 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 11:03:03,063 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 11:03:03,214 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 11:03:03,289 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 11:03:03,897 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 11:03:04,940 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 11:03:10,321 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 11:03:10,358 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 11:03:14,883 - INFO - ===== Start Transform Customer Data =====
2025-11-14 11:03:14,909 - ERROR - ===== Failed Transform Customer Data =====
2025-11-14 11:03:14,909 - ERROR - name 'when' is not defined
2025-11-14 11:04:05,715 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 11:04:06,598 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 11:04:09,076 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 11:04:09,122 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 11:04:09,291 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 11:04:09,353 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 11:04:09,814 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 11:04:10,729 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 11:04:15,527 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 11:04:15,572 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 11:04:20,285 - INFO - ===== Start Transform Customer Data =====
2025-11-14 11:04:20,379 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 11:04:24,685 - INFO - ===== Start Transform Customer Data =====
2025-11-14 11:04:24,738 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 11:04:26,355 - INFO - ===== Start Load data to the database =====
2025-11-14 11:04:26,626 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:04:26,659 - INFO - ===== Start Load data to the database =====
2025-11-14 11:04:26,770 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:04:26,801 - INFO - ===== Start Load data to the database =====
2025-11-14 11:04:28,836 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:04:28,864 - INFO - ===== Start Load data to the database =====
2025-11-14 11:04:32,358 - ERROR - ===== Failed Load data to the database =====
2025-11-14 11:04:32,358 - ERROR - An error occurred while calling o348.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 106.0 failed 1 times, most recent failure: Lost task 3.0 in stage 106.0 (TID 141) (pyspark executor driver): java.sql.BatchUpdateException: Batch entry 722 INSERT INTO customers ("customer_id","birth_date","gender","location","account_balance") VALUES ('C3380368','1800-01-01 +00','T','CHENNAI',50050.00) was aborted: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C3380368, 1800-01-01, T, CHENNAI, 50050, 2025-11-14 11:04:29.110232, 2025-11-14 11:04:29.110232).  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.postgresql.util.PSQLException: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C3380368, 1800-01-01, T, CHENNAI, 50050, 2025-11-14 11:04:29.110232, 2025-11-14 11:04:29.110232).
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.BatchUpdateException: Batch entry 722 INSERT INTO customers ("customer_id","birth_date","gender","location","account_balance") VALUES ('C3380368','1800-01-01 +00','T','CHENNAI',50050.00) was aborted: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C3380368, 1800-01-01, T, CHENNAI, 50050, 2025-11-14 11:04:29.110232, 2025-11-14 11:04:29.110232).  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.postgresql.util.PSQLException: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C3380368, 1800-01-01, T, CHENNAI, 50050, 2025-11-14 11:04:29.110232, 2025-11-14 11:04:29.110232).
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

2025-11-14 11:10:16,452 - INFO - ===== Start Transform Customer Data =====
2025-11-14 11:10:16,546 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 11:10:41,120 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 11:10:42,108 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 11:10:44,349 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 11:10:44,390 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 11:10:44,512 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 11:10:44,562 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 11:10:45,011 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 11:10:45,832 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 11:10:50,903 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 11:10:50,940 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 11:10:55,074 - INFO - ===== Start Transform Customer Data =====
2025-11-14 11:10:55,138 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 11:10:58,731 - INFO - ===== Start Transform Customer Data =====
2025-11-14 11:10:58,781 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 11:10:59,117 - INFO - ===== Start Load data to the database =====
2025-11-14 11:10:59,338 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:10:59,368 - INFO - ===== Start Load data to the database =====
2025-11-14 11:10:59,478 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:10:59,507 - INFO - ===== Start Load data to the database =====
2025-11-14 11:11:01,605 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:11:01,652 - INFO - ===== Start Load data to the database =====
2025-11-14 11:11:05,236 - ERROR - ===== Failed Load data to the database =====
2025-11-14 11:11:05,236 - ERROR - An error occurred while calling o350.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 102.0 failed 1 times, most recent failure: Lost task 3.0 in stage 102.0 (TID 125) (pyspark executor driver): java.sql.BatchUpdateException: Batch entry 722 INSERT INTO customers ("customer_id","birth_date","gender","location","account_balance") VALUES ('C3380368','1800-01-01 +00','T','CHENNAI',50050.00) was aborted: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C3380368, 1800-01-01, T, CHENNAI, 50050, 2025-11-14 11:11:01.990676, 2025-11-14 11:11:01.990676).  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.postgresql.util.PSQLException: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C3380368, 1800-01-01, T, CHENNAI, 50050, 2025-11-14 11:11:01.990676, 2025-11-14 11:11:01.990676).
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.BatchUpdateException: Batch entry 722 INSERT INTO customers ("customer_id","birth_date","gender","location","account_balance") VALUES ('C3380368','1800-01-01 +00','T','CHENNAI',50050.00) was aborted: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C3380368, 1800-01-01, T, CHENNAI, 50050, 2025-11-14 11:11:01.990676, 2025-11-14 11:11:01.990676).  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.postgresql.util.PSQLException: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C3380368, 1800-01-01, T, CHENNAI, 50050, 2025-11-14 11:11:01.990676, 2025-11-14 11:11:01.990676).
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

2025-11-14 11:22:43,793 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 11:22:44,764 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 11:22:48,100 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 11:22:48,155 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 11:22:48,305 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 11:22:48,377 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 11:22:48,914 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 11:22:49,812 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 11:23:39,334 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 11:23:40,234 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 11:23:42,525 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 11:23:42,577 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 11:23:42,741 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 11:23:42,797 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 11:23:43,176 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 11:23:44,001 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 11:24:41,130 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 11:24:41,932 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 11:24:44,012 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 11:24:44,048 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 11:24:44,180 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 11:24:44,240 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 11:24:44,632 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 11:24:45,410 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 11:40:59,252 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 11:41:01,011 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 11:41:03,707 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 11:41:03,768 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 11:41:03,900 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 11:41:03,955 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 11:41:04,469 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 11:41:05,390 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 11:41:20,174 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 11:41:20,204 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 11:41:24,486 - INFO - ===== Start Transform Customer Data =====
2025-11-14 11:41:24,588 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 11:41:29,086 - INFO - ===== Start Transform Customer Data =====
2025-11-14 11:41:29,208 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 11:41:29,740 - INFO - ===== Start Load data to the database =====
2025-11-14 11:41:30,011 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:41:30,049 - INFO - ===== Start Load data to the database =====
2025-11-14 11:41:30,165 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:41:30,197 - INFO - ===== Start Load data to the database =====
2025-11-14 11:41:32,178 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:41:32,239 - INFO - ===== Start Load data to the database =====
2025-11-14 11:41:42,406 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:41:42,458 - INFO - ===== Start Load data to the database =====
2025-11-14 11:41:47,131 - ERROR - ===== Failed Load data to the database =====
2025-11-14 11:41:47,131 - ERROR - An error occurred while calling o476.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 172.0 failed 1 times, most recent failure: Lost task 3.0 in stage 172.0 (TID 283) (pyspark executor driver): java.sql.BatchUpdateException: Batch entry 602 INSERT INTO transactions ("transaction_id","customer_id","transaction_date","transaction_time","transaction_amount") VALUES ('T959988','C3380368','2016-09-10 +00','17:07:31',32500.00) was aborted: ERROR: insert or update on table "transactions" violates foreign key constraint "transactions_customer_id_fkey"
  Detail: Key (customer_id)=(C3380368) is not present in table "customers".  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.postgresql.util.PSQLException: ERROR: insert or update on table "transactions" violates foreign key constraint "transactions_customer_id_fkey"
  Detail: Key (customer_id)=(C3380368) is not present in table "customers".
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.BatchUpdateException: Batch entry 602 INSERT INTO transactions ("transaction_id","customer_id","transaction_date","transaction_time","transaction_amount") VALUES ('T959988','C3380368','2016-09-10 +00','17:07:31',32500.00) was aborted: ERROR: insert or update on table "transactions" violates foreign key constraint "transactions_customer_id_fkey"
  Detail: Key (customer_id)=(C3380368) is not present in table "customers".  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.postgresql.util.PSQLException: ERROR: insert or update on table "transactions" violates foreign key constraint "transactions_customer_id_fkey"
  Detail: Key (customer_id)=(C3380368) is not present in table "customers".
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

2025-11-14 11:44:11,654 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 11:44:12,809 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 11:44:15,641 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 11:44:15,676 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 11:44:15,810 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 11:44:15,873 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 11:44:16,262 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 11:44:17,166 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 11:44:32,171 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 11:44:32,208 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 11:44:35,943 - INFO - ===== Start Transform Customer Data =====
2025-11-14 11:44:36,064 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 11:44:40,211 - INFO - ===== Start Transform Customer Data =====
2025-11-14 11:44:40,277 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 11:44:40,728 - INFO - ===== Start Load data to the database =====
2025-11-14 11:44:40,947 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:44:40,982 - INFO - ===== Start Load data to the database =====
2025-11-14 11:44:41,102 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:44:41,131 - INFO - ===== Start Load data to the database =====
2025-11-14 11:44:43,193 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:44:43,358 - INFO - ===== Start Load data to the database =====
2025-11-14 11:44:54,213 - INFO - ===== Finish Load data to the database =====
2025-11-14 11:44:54,258 - INFO - ===== Start Load data to the database =====
2025-11-14 11:44:58,909 - ERROR - ===== Failed Load data to the database =====
2025-11-14 11:44:58,910 - ERROR - An error occurred while calling o475.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 172.0 failed 1 times, most recent failure: Lost task 3.0 in stage 172.0 (TID 283) (pyspark executor driver): java.sql.BatchUpdateException: Batch entry 602 INSERT INTO transactions ("transaction_id","customer_id","transaction_date","transaction_time","transaction_amount") VALUES ('T959988','C3380368','2016-09-10 +00','17:07:31',32500.00) was aborted: ERROR: insert or update on table "transactions" violates foreign key constraint "transactions_customer_id_fkey"
  Detail: Key (customer_id)=(C3380368) is not present in table "customers".  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.postgresql.util.PSQLException: ERROR: insert or update on table "transactions" violates foreign key constraint "transactions_customer_id_fkey"
  Detail: Key (customer_id)=(C3380368) is not present in table "customers".
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.BatchUpdateException: Batch entry 602 INSERT INTO transactions ("transaction_id","customer_id","transaction_date","transaction_time","transaction_amount") VALUES ('T959988','C3380368','2016-09-10 +00','17:07:31',32500.00) was aborted: ERROR: insert or update on table "transactions" violates foreign key constraint "transactions_customer_id_fkey"
  Detail: Key (customer_id)=(C3380368) is not present in table "customers".  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.postgresql.util.PSQLException: ERROR: insert or update on table "transactions" violates foreign key constraint "transactions_customer_id_fkey"
  Detail: Key (customer_id)=(C3380368) is not present in table "customers".
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

2025-11-14 11:54:03,810 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 11:54:04,961 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 11:54:07,871 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 11:54:07,922 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 11:54:08,089 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 11:54:08,157 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 11:54:08,684 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 11:54:09,562 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 12:14:03,900 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 12:14:05,276 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 13:19:47,770 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 13:19:49,066 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 13:19:52,374 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 13:19:52,450 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 13:19:52,568 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 13:19:52,629 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 13:19:53,089 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 13:19:54,098 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 13:20:52,616 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 13:20:53,348 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 13:20:55,452 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 13:20:55,497 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 13:20:55,650 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 13:20:55,706 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 13:20:56,066 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 13:20:56,817 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 13:21:10,784 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 13:21:10,812 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 13:21:12,881 - INFO - ===== Start Transform Customer Data =====
2025-11-14 13:21:13,025 - ERROR - ===== Failed Transform Customer Data =====
2025-11-14 13:21:13,025 - ERROR - [MISSING_ATTRIBUTES.RESOLVED_ATTRIBUTE_MISSING_FROM_INPUT] Resolved attribute(s) "gender" missing from "TransactionID", "CustomerID", "CustomerDOB", "CustGender", "CustLocation", "CustAccountBalance", "TransactionDate", "TransactionTime", "TransactionAmount (INR)" in operator !Filter gender#1726 IN (M,F). ;
!Filter gender#1726 IN (M,F)
+- Filter atleastnnonnulls(9, TransactionID#188, CustomerID#189, CustomerDOB#190, CustGender#191, CustLocation#192, CustAccountBalance#193, TransactionDate#194, TransactionTime#195, TransactionAmount (INR)#196)
   +- Filter atleastnnonnulls(9, TransactionID#188, CustomerID#189, CustomerDOB#190, CustGender#191, CustLocation#192, CustAccountBalance#193, TransactionDate#194, TransactionTime#195, TransactionAmount (INR)#196)
      +- Relation [TransactionID#188,CustomerID#189,CustomerDOB#190,CustGender#191,CustLocation#192,CustAccountBalance#193,TransactionDate#194,TransactionTime#195,TransactionAmount (INR)#196] csv

2025-11-14 13:28:59,759 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 13:29:00,810 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 13:29:03,322 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 13:29:03,376 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 13:29:03,513 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 13:29:03,569 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 13:29:04,002 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 13:29:05,025 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 13:29:19,070 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 13:29:19,118 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 13:29:21,732 - INFO - ===== Start Transform Customer Data =====
2025-11-14 13:29:21,758 - ERROR - ===== Failed Transform Customer Data =====
2025-11-14 13:29:21,759 - ERROR - name 'cust_df' is not defined
2025-11-14 13:30:01,550 - INFO - ===== Start Transform Customer Data =====
2025-11-14 13:30:01,563 - ERROR - ===== Failed Transform Customer Data =====
2025-11-14 13:30:01,563 - ERROR - name 'cust_df' is not defined
2025-11-14 13:30:37,278 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 13:30:38,223 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 13:30:40,615 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 13:30:40,657 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 13:30:40,804 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 13:30:40,858 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 13:30:41,237 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 13:30:42,090 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 13:30:56,407 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 13:30:56,442 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 13:30:59,246 - INFO - ===== Start Transform Customer Data =====
2025-11-14 13:30:59,268 - ERROR - ===== Failed Transform Customer Data =====
2025-11-14 13:30:59,269 - ERROR - name 'cust_df' is not defined
2025-11-14 13:31:41,098 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 13:31:41,824 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 13:31:43,919 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 13:31:43,964 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 13:31:44,092 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 13:31:44,144 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 13:31:44,473 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 13:31:45,314 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 13:31:58,704 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 13:31:58,738 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 13:32:01,151 - INFO - ===== Start Transform Customer Data =====
2025-11-14 13:32:01,228 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 13:34:19,869 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 13:34:20,599 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 13:34:22,655 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 13:34:22,692 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 13:34:22,801 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 13:34:22,852 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 13:34:23,168 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 13:34:23,939 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 13:34:37,457 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 13:34:37,501 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 13:34:39,941 - INFO - ===== Start Transform Customer Data =====
2025-11-14 13:34:40,020 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 13:34:44,133 - INFO - ===== Start Transform Customer Data =====
2025-11-14 13:34:44,178 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 13:34:44,665 - INFO - ===== Start Load data to the database =====
2025-11-14 13:34:44,874 - INFO - ===== Finish Load data to the database =====
2025-11-14 13:34:44,912 - INFO - ===== Start Load data to the database =====
2025-11-14 13:34:45,055 - INFO - ===== Finish Load data to the database =====
2025-11-14 13:34:45,092 - INFO - ===== Start Load data to the database =====
2025-11-14 13:34:47,221 - INFO - ===== Finish Load data to the database =====
2025-11-14 13:34:47,263 - INFO - ===== Start Load data to the database =====
2025-11-14 13:34:56,155 - INFO - ===== Finish Load data to the database =====
2025-11-14 13:34:56,204 - INFO - ===== Start Load data to the database =====
2025-11-14 13:34:56,341 - ERROR - ===== Failed Load data to the database =====
2025-11-14 13:34:56,342 - ERROR - Column birth_date not found in schema Some(StructType(StructField(transaction_id,StringType,false),StructField(customer_id,StringType,true),StructField(transaction_date,DateType,true),StructField(transaction_time,StringType,true),StructField(transaction_amount,DoubleType,true),StructField(created_at,TimestampType,true),StructField(updated_at,TimestampType,true))).
2025-11-14 13:36:12,267 - INFO - ===== Start Transform Customer Data =====
2025-11-14 13:36:12,353 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 13:36:19,246 - INFO - ===== Start Load data to the database =====
2025-11-14 13:36:32,212 - INFO - ===== Finish Load data to the database =====
2025-11-14 13:38:41,255 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 13:38:42,422 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 13:47:24,403 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 13:47:25,907 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 13:47:28,588 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 13:47:28,659 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 13:47:28,842 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 13:47:28,911 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 13:47:29,442 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 13:47:30,240 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 13:47:44,846 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 13:47:44,879 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 13:48:42,666 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 13:48:43,608 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 13:48:46,202 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 13:48:46,262 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 13:48:46,451 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 13:48:46,540 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 13:48:47,128 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 13:48:48,013 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 13:49:01,294 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 13:49:01,328 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 13:49:03,912 - INFO - ===== Start Transform Customer Data =====
2025-11-14 13:49:04,015 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 13:49:08,156 - INFO - ===== Start Transform Customer Data =====
2025-11-14 13:49:08,217 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 13:49:08,759 - INFO - ===== Start Load data to the database =====
2025-11-14 13:49:08,950 - INFO - ===== Finish Load data to the database =====
2025-11-14 13:49:08,985 - INFO - ===== Start Load data to the database =====
2025-11-14 13:49:09,080 - INFO - ===== Finish Load data to the database =====
2025-11-14 13:49:09,111 - INFO - ===== Start Load data to the database =====
2025-11-14 13:49:11,043 - INFO - ===== Finish Load data to the database =====
2025-11-14 13:49:11,372 - INFO - ===== Start Load data to the database =====
2025-11-14 13:49:21,721 - INFO - ===== Finish Load data to the database =====
2025-11-14 13:49:21,770 - INFO - ===== Start Load data to the database =====
2025-11-14 13:49:36,143 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:18:37,174 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 14:18:38,128 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 14:18:38,132 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 14:18:38,182 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 14:18:38,186 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 14:18:38,249 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 14:18:38,253 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 14:18:40,929 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 14:18:40,929 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 14:18:41,030 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 14:18:41,031 - INFO - ===== Start Transform Customer Data =====
2025-11-14 14:18:41,201 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 14:18:41,218 - INFO - ===== Start Transform Customer Data =====
2025-11-14 14:18:41,295 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 14:18:41,519 - INFO - ===== Start Load data to the database =====
2025-11-14 14:18:42,136 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:18:42,173 - INFO - ===== Start Load data to the database =====
2025-11-14 14:18:42,411 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:18:42,437 - INFO - ===== Start Load data to the database =====
2025-11-14 14:18:45,869 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:18:45,905 - INFO - ===== Start Load data to the database =====
2025-11-14 14:18:55,817 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:18:55,839 - INFO - ===== Start Load data to the database =====
2025-11-14 14:19:08,177 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:19:08,183 - INFO - Closing down clientserver connection
2025-11-14 14:19:47,604 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 14:19:48,419 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 14:19:48,427 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 14:19:48,465 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 14:19:48,468 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 14:19:48,518 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 14:19:48,522 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 14:19:51,013 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 14:19:51,014 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 14:19:51,105 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 14:19:51,106 - INFO - ===== Start Transform Customer Data =====
2025-11-14 14:19:51,240 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 14:19:51,254 - INFO - ===== Start Transform Customer Data =====
2025-11-14 14:19:51,313 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 14:19:51,432 - INFO - ===== Start Load data to the database =====
2025-11-14 14:19:51,867 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:19:51,890 - INFO - ===== Start Load data to the database =====
2025-11-14 14:19:52,062 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:19:52,084 - INFO - ===== Start Load data to the database =====
2025-11-14 14:19:54,395 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:19:54,513 - INFO - ===== Start Load data to the database =====
2025-11-14 14:20:05,488 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:20:05,514 - INFO - ===== Start Load data to the database =====
2025-11-14 14:20:21,510 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:20:21,517 - INFO - Closing down clientserver connection
2025-11-14 14:35:06,224 - INFO - ===== Start Transform Customer Data =====
2025-11-14 14:35:07,705 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 14:35:49,182 - INFO - ===== Start Extracting education_status data (Database) =====
2025-11-14 14:35:50,687 - INFO - ===== Finish Extracting education_status data =====
2025-11-14 14:35:53,882 - INFO - ===== Start Extracting marital_status data (Database) =====
2025-11-14 14:35:53,970 - INFO - ===== Finish Extracting marital_status data =====
2025-11-14 14:35:54,230 - INFO - ===== Start Extracting marketing_campaign_deposit data (Database) =====
2025-11-14 14:35:54,331 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-11-14 14:35:54,991 - INFO - ===== Start Extracting new_bank_transaction data (CSV) =====
2025-11-14 14:35:56,121 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-11-14 14:36:17,748 - INFO - ===== Start Transform Marketing Data =====
2025-11-14 14:36:17,838 - INFO - ===== Finish Transform Marketing Data =====
2025-11-14 14:36:22,267 - INFO - ===== Start Transform Customer Data =====
2025-11-14 14:36:22,511 - INFO - ===== Finish Transform Customer Data =====
2025-11-14 14:36:28,773 - INFO - ===== Start Transform Customer Data =====
2025-11-14 14:36:28,840 - INFO - ===== Finish Transform Transaction Data =====
2025-11-14 14:36:29,577 - INFO - ===== Start Load data to the database =====
2025-11-14 14:36:29,940 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:36:29,987 - INFO - ===== Start Load data to the database =====
2025-11-14 14:36:30,146 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:36:30,202 - INFO - ===== Start Load data to the database =====
2025-11-14 14:36:32,843 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:36:33,338 - INFO - ===== Start Load data to the database =====
2025-11-14 14:36:46,992 - INFO - ===== Finish Load data to the database =====
2025-11-14 14:36:47,101 - INFO - ===== Start Load data to the database =====
2025-11-14 14:37:06,184 - INFO - ===== Finish Load data to the database =====
